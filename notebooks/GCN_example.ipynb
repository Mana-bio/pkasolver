{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkasolver import data\n",
    "from pkasolver import chem\n",
    "from pkasolver import ml\n",
    "from pkasolver import stat\n",
    "from pkasolver import constants as c\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Model package imports\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch import optim\n",
    "\n",
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDFs to dict of preprocessed DataFrames\n",
    "path = \"../data/Baltruschat/\"\n",
    "sdf_training = \"combined_training_datasets_unique.sdf\"\n",
    "sdf_novartis = \"novartis_cleaned_mono_unique_notraindata.sdf\"\n",
    "sdf_AvLiLuMoVe = \"AvLiLuMoVe_cleaned_mono_unique_notraindata.sdf\"\n",
    "\n",
    "datasets = {\n",
    "    \"Training\": path+sdf_training,\n",
    "    \"Novartis\": path+sdf_novartis,\n",
    "    \"AvLiLuMoVe\": path+sdf_AvLiLuMoVe,\n",
    "}\n",
    "\n",
    "pd_datasets = data.preprocess_all(datasets, title='pd_all_datasets')\n",
    "with open(f'data/pd_all_datasets.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(pd_datasets,pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "node_features = [\n",
    "    'atomic_number',\n",
    "    'formal_charge',\n",
    "    'chiral_tag',\n",
    "    'hybridization',\n",
    "    'explicit_Hs_number',\n",
    "    'aromatic_tag',\n",
    "    'total_valence',\n",
    "    'total_degree'\n",
    "]\n",
    "\n",
    "edge_features = [\n",
    "#     'bond_type',\n",
    "#     'is_conjugated'\n",
    "]\n",
    "\n",
    "train_test_split = 0.8\n",
    "batch_size = 64\n",
    "learning_rate=0.001\n",
    "num_epochs = 40000\n",
    "device=\"cpu\"\n",
    "checkpoint_path = 'modelsaves/1#/'\n",
    "paired_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dict of DataFrames\n",
    "with open('data/pd_all_datasets.pkl', 'rb') as pickle_file:\n",
    "    pd_dataset = pickle.load(pickle_file)\n",
    "\n",
    "# make pyG Dataset form 'Training'- Dataset    \n",
    "dataset = data.make_pyg_dataset(pd_dataset['Training'], node_features, edge_features, paired=paired_model)\n",
    "# print(dataset[0], '\\n\\n' ,dataset[0].x,'\\n\\n', dataset[0].edge_index, dataset[0].y,dataset[0].edge_attr)\n",
    "# Split dataset\n",
    "train_data, test_data = ml.pyg_split(dataset, train_test_split, shuffle=True)\n",
    "# Make loaders\n",
    "train_loader = ml.dataset_to_dataloader(train_data, batch_size, shuffle=True)\n",
    "test_loader = ml.dataset_to_dataloader(test_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyg_crossval_split(dataset,num_crossval=5, shuffle=False):\n",
    "    \"\"\"Take List of PyG Data oojcts and a split ratio between 0 and 1 \n",
    "    and return a list of Training data and a list of test data.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(dataset)\n",
    "    \n",
    "    split_length=int(len(dataset)/num_crossval)\n",
    "    data_list = []\n",
    "    for i in range(num_crossval):\n",
    "        print(split_length*(i+1))\n",
    "        data_list.append(dataset[split_length*i:split_length*(i+1)])\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Testsets:\n",
    "pd_dataset['Novartis'].head(2)\n",
    "\n",
    "# make pyG Dataset form 'Training'- Dataset    \n",
    "dataset_nov = data.make_pyg_dataset(pd_dataset['Novartis'], node_features, edge_features, paired=paired_model)\n",
    "# print(dataset_nov[0].ID,dataset_nov[0], '\\n\\n' ,dataset_nov[0].x,'\\n\\n', dataset_nov[0].edge_index, dataset_nov[0].y)\n",
    "nov_loader = ml.dataset_to_dataloader(dataset_nov, batch_size)\n",
    "\n",
    "dataset_avli = data.make_pyg_dataset(pd_dataset['AvLiLuMoVe'], node_features, edge_features, paired=paired_model)\n",
    "# print(dataset_avli[0].ID,dataset_avli[0], '\\n\\n' ,dataset_avli[0].x,'\\n\\n', dataset_avli[0].edge_index, dataset_avli[0].y)\n",
    "avli_loader = ml.dataset_to_dataloader(dataset_avli, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict={\n",
    "    'paired':paired_model,\n",
    "    'num_graph_layers':4,\n",
    "    'hidden_channels':96,\n",
    "    'num_linear_layers':1,\n",
    "    'edge_nn_layer':None,\n",
    "    'num_node_features':dataset[0].num_features,\n",
    "    'num_edge_features':dataset[0].num_edge_features\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Single Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "\n",
    "#Model Class:\n",
    "\n",
    "def convs(parameter_dict):\n",
    "    num_graph_layers = parameter_dict['num_graph_layers']\n",
    "    num_linear_layers = parameter_dict['num_linear_layers']\n",
    "    num_node_features = parameter_dict['num_node_features']\n",
    "    num_edge_features = parameter_dict['num_edge_features']\n",
    "    hidden_channels=parameter_dict['hidden_channels']\n",
    "    paired = parameter_dict['paired']\n",
    "    \n",
    "    if num_edge_features > 0:\n",
    "        nn = Seq(Lin(num_edge_features, 16), ReLU(), Lin(16, num_node_features*hidden_channels))\n",
    "        nn1 = Seq(Lin(num_edge_features, 16), ReLU(), Lin(16, hidden_channels* hidden_channels))\n",
    "        convs = ModuleList([NNConv(num_node_features, hidden_channels, nn=nn)])\n",
    "        convs.extend([NNConv(hidden_channels, hidden_channels, nn=nn1) for i in range(num_graph_layers-1)])\n",
    "    else:\n",
    "        convs = ModuleList([GCNConv(num_node_features, hidden_channels)])\n",
    "        convs.extend([GCNConv(hidden_channels, hidden_channels) for i in range(num_graph_layers-1)])\n",
    "    return convs\n",
    "\n",
    "def lins(parameter_dict):\n",
    "    num_graph_layers = parameter_dict['num_graph_layers']\n",
    "    num_linear_layers = parameter_dict['num_linear_layers']\n",
    "    num_node_features = parameter_dict['num_node_features']\n",
    "    num_edge_features = parameter_dict['num_edge_features']\n",
    "    hidden_channels=parameter_dict['hidden_channels']\n",
    "    paired = parameter_dict['paired']\n",
    "    \n",
    "    channels= hidden_channels\n",
    "    if paired:\n",
    "        channels= hidden_channels*2\n",
    "    lins= ModuleList([Linear(channels, channels) for i in range(num_linear_layers-1)])\n",
    "    lins.extend([Linear(channels, 1)])\n",
    "    return lins\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    num_graph_layers = parameter_dict['num_graph_layers']\n",
    "    num_linear_layers = parameter_dict['num_linear_layers']\n",
    "    num_node_features = parameter_dict['num_node_features']\n",
    "    num_edge_features = parameter_dict['num_edge_features']\n",
    "    hidden_channels=parameter_dict['hidden_channels']\n",
    "    paired = parameter_dict['paired']\n",
    "    \n",
    "    def __init__(self, parameter_dict):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(1)\n",
    "        \n",
    "        num_graph_layers = parameter_dict['num_graph_layers']\n",
    "        num_node_features = parameter_dict['num_node_features']\n",
    "        num_edge_features = parameter_dict['num_edge_features']\n",
    "        hidden_channels=parameter_dict['hidden_channels']\n",
    "        paired = parameter_dict['paired']\n",
    "        \n",
    "        self.convs_x = convs(parameter_dict)\n",
    "        if paired:\n",
    "            self.convs_x2 = convs(parameter_dict)\n",
    "        self.lin = lins(parameter_dict)\n",
    "\n",
    "\n",
    "            \n",
    "    def forward(self, data):\n",
    "        try:\n",
    "            if self.convs_x2:\n",
    "                paired = True\n",
    "        except:\n",
    "            paired = False\n",
    "            \n",
    "        x = data.x\n",
    "        if paired:\n",
    "            x2 = data.x2\n",
    "        # 1. Obtain node embeddings\n",
    "        if len(edge_features) > 0:\n",
    "            for i in range(len(self.convs_x)):\n",
    "                x = self.convs_x[i](x, data.edge_index, data.edge_attr)\n",
    "                x = x.relu()\n",
    "            if paired:\n",
    "                for i in range(len(self.convs_x2)):\n",
    "                    x2 = self.convs_x2[i](x2, data.edge_index2, data.edge_attr2)\n",
    "                    x2 = x2.relu()\n",
    "                \n",
    "        else:\n",
    "            for i in range(len(self.convs_x)):\n",
    "                x = self.convs_x[i](x, data.edge_index)\n",
    "                x = x.relu()\n",
    "            if paired:\n",
    "                for i in range(len(self.convs_x2)):\n",
    "                    x2 = self.convs_x2[i](x2, data.edge_index2)\n",
    "                    x2 = x2.relu()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_max_pool(x, data.x_batch.to(device=device))  # [batch_size, hidden_channels]\n",
    "        if paired: \n",
    "            x2 = global_max_pool(x2, data.x2_batch.to(device=device))\n",
    "            \n",
    "        if paired:\n",
    "            x = torch.cat((x, x2), 1)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        for i in range(len(self.lin)):\n",
    "                x = self.lin[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (convs_x): ModuleList(\n",
      "    (0): GCNConv(8, 96)\n",
      "    (1): GCNConv(96, 96)\n",
      "    (2): GCNConv(96, 96)\n",
      "    (3): GCNConv(96, 96)\n",
      "  )\n",
      "  (lin): ModuleList(\n",
      "    (0): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create Single model\n",
    "model = GCN(parameter_dict).to(device=device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_v = torch.nn.L1Loss() # that's the MAE Loss\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    for data in loader:  # Iterate in batches over the training dataset. \n",
    "        out = model(data)\n",
    "        loss = criterion(out.flatten(), data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad() # Clear gradients.\n",
    "        \n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    loss = torch.Tensor([0]).to(device=device)\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        loss += criterion_v(out.flatten(), data.y)\n",
    "    return loss/len(loader) # MAE loss of batches can be summed and divided by the number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save first checkpoint\n",
    "checkpoint = {\n",
    "    'epoch': 0,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'training_data': [train_data[i].ID for i in range(len(train_data))],\n",
    "    'test_data': [test_data[i].ID for i in range(len(test_data))],\n",
    "    'node_features': node_features,\n",
    "    'edge_features': edge_features,\n",
    "    'progress':'',\n",
    "    'parameters':parameter_dict\n",
    "}\n",
    "    \n",
    "torch.save(checkpoint,f'{checkpoint_path}checkpoint_epoch_0.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load checkpoint \n",
    "checkpoint = torch.load('modelsaves/1#/checkpoint_epoch_0.pth.tar')\n",
    "train_data =data.load_data(dataset, checkpoint['training_data'])\n",
    "test_data =data.load_data(dataset, checkpoint['test_data'])\n",
    "parameter_dict = checkpoint['parameters']\n",
    "\n",
    "model = GCN(parameter_dict).to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(checkpoint['progress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train MAE: 7.1538, Test MAE: 7.2858\n",
      "Saving checkpoint...\n",
      "done\n",
      "Epoch: 002, Train MAE: 1.9661, Test MAE: 1.9509\n",
      "Epoch: 004, Train MAE: 1.8802, Test MAE: 1.8532\n",
      "Epoch: 006, Train MAE: 1.8555, Test MAE: 1.8390\n",
      "Epoch: 008, Train MAE: 1.8470, Test MAE: 1.8325\n",
      "Epoch: 010, Train MAE: 1.8360, Test MAE: 1.8221\n",
      "Epoch: 012, Train MAE: 1.8191, Test MAE: 1.8079\n",
      "Epoch: 014, Train MAE: 1.8015, Test MAE: 1.7809\n",
      "Epoch: 016, Train MAE: 1.8456, Test MAE: 1.8335\n",
      "Epoch: 018, Train MAE: 1.7462, Test MAE: 1.7333\n",
      "Epoch: 020, Train MAE: 1.7435, Test MAE: 1.7311\n",
      "Epoch: 022, Train MAE: 1.7075, Test MAE: 1.6889\n",
      "Epoch: 024, Train MAE: 1.6757, Test MAE: 1.6645\n",
      "Epoch: 026, Train MAE: 1.6654, Test MAE: 1.6561\n",
      "Epoch: 028, Train MAE: 1.6900, Test MAE: 1.6941\n",
      "Epoch: 030, Train MAE: 1.6428, Test MAE: 1.6390\n",
      "Epoch: 032, Train MAE: 1.6079, Test MAE: 1.6084\n",
      "Epoch: 034, Train MAE: 1.5512, Test MAE: 1.5613\n",
      "Epoch: 036, Train MAE: 1.5219, Test MAE: 1.5344\n",
      "Epoch: 038, Train MAE: 1.4468, Test MAE: 1.4735\n",
      "Epoch: 040, Train MAE: 1.3745, Test MAE: 1.4066\n",
      "Saving checkpoint...\n",
      "done\n",
      "Epoch: 042, Train MAE: 1.3342, Test MAE: 1.3811\n",
      "Epoch: 044, Train MAE: 1.4034, Test MAE: 1.4578\n",
      "Epoch: 046, Train MAE: 1.3356, Test MAE: 1.3848\n",
      "Epoch: 048, Train MAE: 1.3019, Test MAE: 1.3590\n",
      "Epoch: 050, Train MAE: 1.2335, Test MAE: 1.2755\n",
      "Epoch: 052, Train MAE: 1.2842, Test MAE: 1.3330\n",
      "Epoch: 054, Train MAE: 1.2077, Test MAE: 1.2673\n",
      "Epoch: 056, Train MAE: 1.2604, Test MAE: 1.3241\n",
      "Epoch: 058, Train MAE: 1.1942, Test MAE: 1.2557\n",
      "Epoch: 060, Train MAE: 1.2858, Test MAE: 1.3176\n",
      "Epoch: 062, Train MAE: 1.1723, Test MAE: 1.2332\n",
      "Epoch: 064, Train MAE: 1.1954, Test MAE: 1.2542\n",
      "Epoch: 066, Train MAE: 1.1298, Test MAE: 1.1906\n",
      "Epoch: 068, Train MAE: 1.1241, Test MAE: 1.1900\n",
      "Epoch: 070, Train MAE: 1.0923, Test MAE: 1.1591\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a18bda4e2257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Epoch: {epoch:03d}, Train MAE: {train_acc.item():.4f}, Test MAE: {test_acc.item():.4f}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-37b3c3ce7c06>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate in batches over the training dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Perform a single forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# MAE loss of batches can be summed and divided by the number of batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a82b07b861f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpaired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n\u001b[0m\u001b[1;32m    183\u001b[0m                              size=None)\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Otherwise, run both functions in separation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             coll_dict = self.__collect__(self.__user_args__, edge_index, size,\n\u001b[0m\u001b[1;32m    234\u001b[0m                                          kwargs)\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36m__collect__\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_size__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                     data = self.__lift__(data, edge_index,\n\u001b[0m\u001b[1;32m    158\u001b[0m                                          j if arg[-2:] == '_j' else i)\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36m__lift__\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(checkpoint['epoch'], num_epochs):\n",
    "    if epoch != 0: \n",
    "        train(train_loader)\n",
    "    if epoch % 2 == 0:\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        update = f'Epoch: {epoch:03d}, Train MAE: {train_acc.item():.4f}, Test MAE: {test_acc.item():.4f}'\n",
    "        print(update)\n",
    "    if epoch % 40 ==0:\n",
    "        checkpoint = ml.update_checkpoint(checkpoint, epoch, optimizer, update, model)\n",
    "        print('Saving checkpoint...')\n",
    "        torch.save(checkpoint,f'{checkpoint_path}checkpoint_epoch_{epoch}.pth.tar')\n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def test_GCN(loader, dataset_name):\n",
    "    model.eval()\n",
    "    i = 0\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "        y_pred = model(data).reshape(-1)\n",
    "        y_true = data.y\n",
    "        if i == 0:\n",
    "            Y_pred = y_pred \n",
    "            Y_true = y_true\n",
    "        else:\n",
    "            Y_true=torch.hstack((Y_true,y_true))\n",
    "            Y_pred=torch.hstack((Y_pred,y_pred))\n",
    "        i+=1\n",
    "    return pd.DataFrame({'Dataset':dataset_name,'pKa_experimental':Y_true.detach().numpy(), 'pKa_predicted':Y_pred.detach().numpy()})\n",
    "\n",
    "def f(x):\n",
    "    d = {}\n",
    "    d['r2_score'] = r2_score(x['pKa_experimental'],x['pKa_predicted'])\n",
    "    d['MAE'] = mean_absolute_error(x['pKa_experimental'],x['pKa_predicted'])\n",
    "    d['RMSE'] = np.sqrt(mean_squared_error(x['pKa_experimental'],x['pKa_predicted']))\n",
    "    return pd.Series(d, index=['r2_score', 'MAE', 'RMSE']).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2_score</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Literature_set</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.835</td>\n",
       "      <td>1.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Novartis_set</th>\n",
       "      <td>0.416</td>\n",
       "      <td>1.430</td>\n",
       "      <td>1.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation_set</th>\n",
       "      <td>0.590</td>\n",
       "      <td>1.218</td>\n",
       "      <td>1.544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                r2_score    MAE   RMSE\n",
       "Dataset                               \n",
       "Literature_set     0.790  0.835  1.080\n",
       "Novartis_set       0.416  1.430  1.760\n",
       "Validation_set     0.590  1.218  1.544"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([test_GCN(test_loader,'Validation_set'),\n",
    "         test_GCN(nov_loader,'Novartis_set'),\n",
    "         test_GCN(avli_loader,'Literature_set')])\n",
    "\n",
    "df.groupby('Dataset').apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "from captum.attr import NeuronConductance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.linalg import block_diag\n",
    "#####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_importances(ig, dataset, sample_size, node_feature_names, edge_feature_names=None, device='cpu'):\n",
    "    \"\"\"Take Integrated Gradients Object, PyG Dataset and desired sample size. \n",
    "    Return two np.arrays of importances of nodes and edges, respectivly.\n",
    "    \"\"\"\n",
    "    if edge_feature_names == None:\n",
    "        edge_feature_names=[]    \n",
    "    feature_names= node_feature_names + edge_feature_names \n",
    "    attr = np.empty((0,len(feature_names)))\n",
    "    ids = []\n",
    "    \n",
    "    i = 0\n",
    "    for input_data in random.sample(dataset, sample_size):\n",
    "        ids.extend([input_data.ID]*(input_data.x.shape[0]+input_data.edge_index.shape[1]))\n",
    "        if edge_feature_names==[]:\n",
    "#             _attr, _delta = ig.attribute((input_data.x),\n",
    "#                                          additional_forward_args=(input_data.edge_attr, input_data.edge_index, torch.zeros(input_data.x.shape[0], dtype=int).to(device=device)), \n",
    "#                                          internal_batch_size=input_data.x.shape[0], \n",
    "#                                          return_convergence_delta=True)\n",
    "            _attr, _delta = ig.attribute((input_data),\n",
    "                                         additional_forward_args=(input_data.edge_attr, input_data.edge_index, torch.zeros(input_data.x.shape[0], dtype=int).to(device=device)), \n",
    "                                         internal_batch_size=input_data.x.shape[0], \n",
    "                                         return_convergence_delta=True)\n",
    "            attr_row = _attr.detach().numpy()\n",
    "        else:\n",
    "            _attr, _delta = ig.attribute((input_data.x, input_data.edge_attr),\n",
    "                                         additional_forward_args=(input_data.edge_index, torch.zeros(input_data.x.shape[0], dtype=int).to(device=device)), \n",
    "                                         internal_batch_size=input_data.x.shape[0], \n",
    "                                         return_convergence_delta=True)\n",
    "\n",
    "            attr_row = block_diag(_attr[0].detach().numpy(), _attr[1].detach().numpy())\n",
    "#             print(attr_row)\n",
    "        attr = np.vstack((attr, attr_row))\n",
    "        \n",
    "        if i%5==0:\n",
    "            print(f'{i+1} of {sample_size}')\n",
    "        i += 1\n",
    "    \n",
    "    df =pd.DataFrame(attr,columns=feature_names)\n",
    "    df.insert(0, 'ID', ids)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "`inputs` must have type torch.Tensor but <class 'torch_geometric.data.data.Data'> found: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ab05f5a654ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntegratedGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mattr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-5839bea1a3f9>\u001b[0m in \u001b[0;36mcalc_importances\u001b[0;34m(ig, dataset, sample_size, node_feature_names, edge_feature_names, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#                                          internal_batch_size=input_data.x.shape[0],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#                                          return_convergence_delta=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             _attr, _delta = ig.attribute((input_data),\n\u001b[0m\u001b[1;32m     20\u001b[0m                                          \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                          \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mis_inputs_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_input_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/captum/attr/_utils/common.py\u001b[0m in \u001b[0;36m_format_input_baseline\u001b[0;34m(inputs, baselines)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaselineType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m ) -> Tuple[Tuple[Tensor, ...], Tuple[Union[Tensor, int, float], ...]]:\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mbaselines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_format_input\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_format_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_format_tensor_into_tuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pka38/lib/python3.8/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_format_tensor_into_tuples\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         assert isinstance(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         ), \"`inputs` must have type \" \"torch.Tensor but {} found: \".format(type(inputs))\n",
      "\u001b[0;31mAssertionError\u001b[0m: `inputs` must have type torch.Tensor but <class 'torch_geometric.data.data.Data'> found: "
     ]
    }
   ],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "attr_df = calc_importances(ig, dataset, 100, node_features, edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attr_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3f14e5f13424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattr_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'attr_df' is not defined"
     ]
    }
   ],
   "source": [
    "attr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attr_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d8894588d836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## sns.set_context('talk')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviolinplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"variable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"points\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Feature attributions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attr_df' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## sns.set_context('talk')\n",
    "f, ax = plt.subplots(figsize=(7, 8))\n",
    "sns.violinplot(y=\"variable\", x=\"value\", data=pd.melt(attr_df.drop(['ID'], axis=1)),inner=\"points\", scale='count')\n",
    "ax.set(xlabel='Feature attributions', ylabel='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
