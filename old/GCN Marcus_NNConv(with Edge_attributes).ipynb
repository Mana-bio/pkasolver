{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkasolver as ps\n",
    "from pkasolver import util\n",
    "from pkasolver import analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "import random\n",
    "\n",
    "def mol_to_pyg(prot):\n",
    "    \"\"\"Take protonated molecules and return a Pytorch Geometric Data object.\"\"\"\n",
    "    i = 0\n",
    "    num_atoms = prot.GetNumAtoms()\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    edges_attr = []\n",
    "\n",
    "    for mol in [prot]:\n",
    "\n",
    "        # ComputeGasteigerCharges(mol)\n",
    "\n",
    "        for atom in mol.GetAtoms():\n",
    "            nodes.append(\n",
    "                np.array(\n",
    "                    [\n",
    "                        #atom.GetIdx() + num_atoms * i,\n",
    "                        #float(atom.GetProp(\"_GasteigerCharge\")),\n",
    "                        atom.GetSymbol() == \"C\",\n",
    "                        atom.GetSymbol() == \"O\",\n",
    "                        atom.GetSymbol() == \"N\",\n",
    "                        atom.GetSymbol() == \"P\",\n",
    "                        atom.GetSymbol() == \"F\",\n",
    "                        atom.GetSymbol() == \"Cl\",\n",
    "                        atom.GetSymbol() == \"I\",\n",
    "                        atom.GetFormalCharge(),\n",
    "                        atom.GetChiralTag(),\n",
    "                        atom.GetHybridization(),\n",
    "                        atom.GetNumExplicitHs(),\n",
    "                        atom.GetIsAromatic(),\n",
    "                        atom.GetTotalValence(),\n",
    "                        atom.GetTotalDegree()\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for bond in mol.GetBonds():\n",
    "            edges.append(\n",
    "                np.array(\n",
    "                    [\n",
    "                        [bond.GetBeginAtomIdx() + num_atoms * i],\n",
    "                        [bond.GetEndAtomIdx() + num_atoms * i],\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            bond_type = [bond.GetBondTypeAsDouble(), bond.GetIsConjugated()]\n",
    "            edges_attr.append(bond_type)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    X = torch.tensor(np.array([np.array(xi) for xi in nodes]), dtype=torch.float)\n",
    "    edge_index = torch.tensor(np.hstack(np.array(edges)), dtype=torch.long)\n",
    "    edge_attr = torch.tensor(np.array(edges_attr), dtype=torch.float)\n",
    "\n",
    "    return Data(x=X, edge_index=edge_index, edge_attr=edge_attr).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pKa marvin_pKa marvin_atom marvin_pKa_type original_dataset       ID  \\\n",
      "0  6.21       6.09          10           basic     ['chembl25']  1702768   \n",
      "1  7.46        8.2           9           basic     ['chembl25']   273537   \n",
      "2   4.2       3.94           9           basic  ['datawarrior']     7175   \n",
      "3  3.73       5.91           8          acidic  ['datawarrior']      998   \n",
      "4  11.0       8.94          13           basic     ['chembl25']   560562   \n",
      "\n",
      "                                  smiles  \\\n",
      "0     Brc1c(NC2CC2)nc(C2CC2)nc1N1CCCCCC1   \n",
      "1      Brc1cc(Br)c(NC2=[NH+]CCN2)c(Br)c1   \n",
      "2                 Brc1cc2cccnc2c2ncccc12   \n",
      "3                Brc1ccc(-c2nn[n-]n2)cc1   \n",
      "4  Brc1ccc(Br)c(N(CC2CC2)C2=[NH+]CCN2)c1   \n",
      "\n",
      "                                          protonated  \\\n",
      "0  <img data-content=\"rdkit/molecule\" src=\"data:i...   \n",
      "1  <img data-content=\"rdkit/molecule\" src=\"data:i...   \n",
      "2  <img data-content=\"rdkit/molecule\" src=\"data:i...   \n",
      "3  <img data-content=\"rdkit/molecule\" src=\"data:i...   \n",
      "4  <img data-content=\"rdkit/molecule\" src=\"data:i...   \n",
      "\n",
      "                                        deprotonated            ka  \n",
      "0  <img data-content=\"rdkit/molecule\" src=\"data:i...  6.165950e-07  \n",
      "1  <img data-content=\"rdkit/molecule\" src=\"data:i...  3.467369e-08  \n",
      "2  <img data-content=\"rdkit/molecule\" src=\"data:i...  6.309573e-05  \n",
      "3  <img data-content=\"rdkit/molecule\" src=\"data:i...  1.862087e-04  \n",
      "4  <img data-content=\"rdkit/molecule\" src=\"data:i...  1.000000e-11  \n"
     ]
    }
   ],
   "source": [
    "data_folder_Bal = \"../data/Baltruschat/\"\n",
    "SDFfile1 = data_folder_Bal + \"combined_training_datasets_unique.sdf\"\n",
    "SDFfile2 = data_folder_Bal + \"novartis_cleaned_mono_unique_notraindata.sdf\"\n",
    "SDFfile3 = data_folder_Bal + \"AvLiLuMoVe_cleaned_mono_unique_notraindata.sdf\"\n",
    "# specify device\n",
    "device = 'cpu'\n",
    "#device = 'cuda'\n",
    "\n",
    "\n",
    "df1 = ps.util.import_sdf(SDFfile1)\n",
    "df2 = ps.util.import_sdf(SDFfile2)\n",
    "df3 = ps.util.import_sdf(SDFfile3)\n",
    "\n",
    "#Data corrections:\n",
    "df1.marvin_atom[90] = \"3\"\n",
    "\n",
    "df1 = util.conjugates_to_DataFrame(df1)\n",
    "df1 = util.sort_conjugates(df1)\n",
    "df1 = util.pka_to_ka(df1)\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[24, 2], edge_index=[2, 24], x=[21, 14], y=[1]) \n",
      "\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 0., 0., 3., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 3., 1., 1., 4., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 0., 1., 3., 2.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 0., 0., 3., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 1., 1.]]) \n",
      "\n",
      " tensor([[ 0,  1,  2,  3,  4,  5,  4,  7,  8,  9, 10, 11, 11, 13, 14,  9, 16, 17,\n",
      "         18,  8,  6, 12, 15, 19],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "         19, 20,  0,  7, 13, 17]])\n"
     ]
    }
   ],
   "source": [
    "#create pyG Dataset\n",
    "dataset = []\n",
    "for i in range(len(df1.index)):\n",
    "    dataset.append(mol_to_pyg(df1.protonated[i]))\n",
    "    dataset[i].y = torch.tensor([float(df1.pKa[i])], dtype=torch.float32, device=device)\n",
    "print(dataset[0], '\\n\\n' ,dataset[0].x,'\\n\\n', dataset[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[24, 2], edge_index=[2, 24], x=[21, 14], y=[1]) \n",
      "\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 0., 0., 3., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 3., 1., 1., 4., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 0., 1., 3., 2.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 0., 0., 3., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 1., 1.]]) \n",
      "\n",
      " tensor([[ 0,  1,  2,  3,  4,  5,  4,  7,  8,  9, 10, 11, 11, 13, 14,  9, 16, 17,\n",
      "         18,  8,  6, 12, 15, 19],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "         19, 20,  0,  7, 13, 17]]) \n",
      "\n",
      " tensor([[1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 1.0000],\n",
      "        [1.5000, 1.0000],\n",
      "        [1.5000, 1.0000],\n",
      "        [1.5000, 1.0000],\n",
      "        [1.5000, 1.0000],\n",
      "        [1.5000, 1.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 1.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.5000, 1.0000],\n",
      "        [1.0000, 0.0000],\n",
      "        [1.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0], '\\n\\n' ,dataset[0].x,'\\n\\n', dataset[0].edge_index,'\\n\\n', dataset[0].edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set Hyperparameters\n",
    "train_test_split = 0.8\n",
    "hidden_channels = 64\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10000\n",
    "\n",
    "#split train and test set\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "split_length=int(len(dataset)*train_test_split)\n",
    "train_dataset = dataset[:split_length]\n",
    "test_dataset = dataset[split_length:]\n",
    "#create Dataloader objects that contain batches \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): NNConv(14, 96)\n",
      "  (conv2): NNConv(96, 64)\n",
      "  (conv3): NNConv(64, 64)\n",
      "  (conv4): NNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 010, Train MAE: 1.5370, Test MAE: 1.5334\n",
      "Epoch: 020, Train MAE: 1.5033, Test MAE: 1.4953\n",
      "Epoch: 030, Train MAE: 1.4335, Test MAE: 1.4479\n",
      "Epoch: 040, Train MAE: 1.1432, Test MAE: 1.2002\n",
      "Epoch: 050, Train MAE: 1.0085, Test MAE: 1.0972\n",
      "Epoch: 060, Train MAE: 1.0152, Test MAE: 1.1110\n",
      "Epoch: 070, Train MAE: 0.9228, Test MAE: 1.0448\n",
      "Epoch: 080, Train MAE: 1.0349, Test MAE: 1.1487\n",
      "Epoch: 090, Train MAE: 0.8310, Test MAE: 0.9904\n",
      "Epoch: 100, Train MAE: 0.8498, Test MAE: 0.9978\n",
      "Epoch: 110, Train MAE: 0.8452, Test MAE: 1.0068\n",
      "Epoch: 120, Train MAE: 0.8988, Test MAE: 1.0765\n",
      "Epoch: 130, Train MAE: 0.7878, Test MAE: 1.0059\n",
      "Epoch: 140, Train MAE: 0.7541, Test MAE: 0.9789\n",
      "Epoch: 150, Train MAE: 0.7173, Test MAE: 0.9762\n",
      "Epoch: 160, Train MAE: 0.7202, Test MAE: 0.9901\n",
      "Epoch: 170, Train MAE: 0.7000, Test MAE: 0.9457\n",
      "Epoch: 180, Train MAE: 0.6784, Test MAE: 0.9461\n",
      "Epoch: 190, Train MAE: 0.6606, Test MAE: 0.9478\n",
      "Epoch: 200, Train MAE: 0.6676, Test MAE: 0.9510\n",
      "Epoch: 210, Train MAE: 0.6526, Test MAE: 0.9449\n",
      "Epoch: 220, Train MAE: 0.6388, Test MAE: 0.9618\n",
      "Epoch: 230, Train MAE: 0.6230, Test MAE: 0.9404\n",
      "Epoch: 240, Train MAE: 0.6305, Test MAE: 0.9304\n",
      "Epoch: 250, Train MAE: 0.6090, Test MAE: 0.9376\n",
      "Epoch: 260, Train MAE: 0.6215, Test MAE: 0.9273\n",
      "Epoch: 270, Train MAE: 0.5942, Test MAE: 0.9343\n",
      "Epoch: 280, Train MAE: 0.5850, Test MAE: 0.9400\n",
      "Epoch: 290, Train MAE: 0.6246, Test MAE: 0.9356\n",
      "Epoch: 300, Train MAE: 0.5907, Test MAE: 0.9360\n",
      "Epoch: 310, Train MAE: 0.5912, Test MAE: 0.9400\n",
      "Epoch: 320, Train MAE: 0.5839, Test MAE: 0.9116\n",
      "Epoch: 330, Train MAE: 0.5691, Test MAE: 0.9314\n",
      "Epoch: 340, Train MAE: 0.5663, Test MAE: 0.9285\n",
      "Epoch: 350, Train MAE: 0.5733, Test MAE: 0.9091\n",
      "Epoch: 360, Train MAE: 0.5546, Test MAE: 0.9082\n",
      "Epoch: 370, Train MAE: 0.5565, Test MAE: 0.9199\n",
      "Epoch: 380, Train MAE: 0.5537, Test MAE: 0.9396\n",
      "Epoch: 390, Train MAE: 0.5550, Test MAE: 0.9247\n",
      "Epoch: 400, Train MAE: 0.5633, Test MAE: 0.9158\n",
      "Epoch: 410, Train MAE: 0.5459, Test MAE: 0.9195\n",
      "Epoch: 420, Train MAE: 0.5204, Test MAE: 0.9102\n",
      "Epoch: 430, Train MAE: 0.5310, Test MAE: 0.9013\n",
      "Epoch: 440, Train MAE: 0.5196, Test MAE: 0.9086\n",
      "Epoch: 450, Train MAE: 0.5204, Test MAE: 0.9140\n",
      "Epoch: 460, Train MAE: 0.5282, Test MAE: 0.9169\n",
      "Epoch: 470, Train MAE: 0.5276, Test MAE: 0.9198\n",
      "Epoch: 480, Train MAE: 0.5253, Test MAE: 0.9137\n",
      "Epoch: 490, Train MAE: 0.5070, Test MAE: 0.9107\n",
      "Epoch: 500, Train MAE: 0.5477, Test MAE: 0.9352\n",
      "Epoch: 510, Train MAE: 0.4968, Test MAE: 0.9133\n",
      "Epoch: 520, Train MAE: 0.5164, Test MAE: 0.9095\n",
      "Epoch: 530, Train MAE: 0.5257, Test MAE: 0.9343\n",
      "Epoch: 540, Train MAE: 0.5108, Test MAE: 0.9137\n",
      "Epoch: 550, Train MAE: 0.5085, Test MAE: 0.8912\n",
      "Epoch: 560, Train MAE: 0.5035, Test MAE: 0.9141\n",
      "Epoch: 570, Train MAE: 0.4913, Test MAE: 0.8994\n",
      "Epoch: 580, Train MAE: 0.4925, Test MAE: 0.8945\n",
      "Epoch: 590, Train MAE: 0.4958, Test MAE: 0.8981\n",
      "Epoch: 600, Train MAE: 0.5003, Test MAE: 0.8873\n",
      "Epoch: 610, Train MAE: 0.4855, Test MAE: 0.8957\n",
      "Epoch: 620, Train MAE: 0.4828, Test MAE: 0.9080\n",
      "Epoch: 630, Train MAE: 0.4988, Test MAE: 0.9067\n",
      "Epoch: 640, Train MAE: 0.4855, Test MAE: 0.8892\n",
      "Epoch: 650, Train MAE: 0.4864, Test MAE: 0.9054\n",
      "Epoch: 660, Train MAE: 0.5023, Test MAE: 0.9047\n",
      "Epoch: 670, Train MAE: 0.5205, Test MAE: 0.9217\n",
      "Epoch: 680, Train MAE: 0.4783, Test MAE: 0.8990\n",
      "Epoch: 690, Train MAE: 0.4893, Test MAE: 0.9194\n",
      "Epoch: 700, Train MAE: 0.4883, Test MAE: 0.8870\n",
      "Epoch: 710, Train MAE: 0.4662, Test MAE: 0.9013\n",
      "Epoch: 720, Train MAE: 0.4834, Test MAE: 0.8994\n",
      "Epoch: 730, Train MAE: 0.4661, Test MAE: 0.8922\n",
      "Epoch: 740, Train MAE: 0.4788, Test MAE: 0.8934\n",
      "Epoch: 750, Train MAE: 0.4909, Test MAE: 0.9034\n",
      "Epoch: 760, Train MAE: 0.4731, Test MAE: 0.8969\n",
      "Epoch: 770, Train MAE: 0.4569, Test MAE: 0.9001\n",
      "Epoch: 780, Train MAE: 0.4645, Test MAE: 0.8894\n",
      "Epoch: 790, Train MAE: 0.4792, Test MAE: 0.8971\n",
      "Epoch: 800, Train MAE: 0.4606, Test MAE: 0.9068\n",
      "Epoch: 810, Train MAE: 0.4764, Test MAE: 0.9034\n",
      "Epoch: 820, Train MAE: 0.4685, Test MAE: 0.8954\n",
      "Epoch: 830, Train MAE: 0.4464, Test MAE: 0.8901\n",
      "Epoch: 840, Train MAE: 0.4714, Test MAE: 0.8852\n",
      "Epoch: 850, Train MAE: 0.4455, Test MAE: 0.8968\n",
      "Epoch: 860, Train MAE: 0.4486, Test MAE: 0.8995\n",
      "Epoch: 870, Train MAE: 0.4679, Test MAE: 0.9060\n",
      "Epoch: 880, Train MAE: 0.4679, Test MAE: 0.8990\n",
      "Epoch: 890, Train MAE: 0.4453, Test MAE: 0.9015\n",
      "Epoch: 900, Train MAE: 0.4661, Test MAE: 0.9034\n",
      "Epoch: 910, Train MAE: 0.4784, Test MAE: 0.9150\n",
      "Epoch: 920, Train MAE: 0.4594, Test MAE: 0.8925\n",
      "Epoch: 930, Train MAE: 0.4512, Test MAE: 0.8813\n",
      "Epoch: 940, Train MAE: 0.4608, Test MAE: 0.8996\n",
      "Epoch: 950, Train MAE: 0.4501, Test MAE: 0.8960\n",
      "Epoch: 960, Train MAE: 0.4655, Test MAE: 0.8907\n",
      "Epoch: 970, Train MAE: 0.4478, Test MAE: 0.8866\n",
      "Epoch: 980, Train MAE: 0.4787, Test MAE: 0.8855\n",
      "Epoch: 990, Train MAE: 0.4648, Test MAE: 0.8900\n",
      "Epoch: 1000, Train MAE: 0.4428, Test MAE: 0.8898\n",
      "Epoch: 1010, Train MAE: 0.4689, Test MAE: 0.8967\n",
      "Epoch: 1020, Train MAE: 0.4345, Test MAE: 0.8857\n",
      "Epoch: 1030, Train MAE: 0.4625, Test MAE: 0.8888\n",
      "Epoch: 1040, Train MAE: 0.4403, Test MAE: 0.8793\n",
      "Epoch: 1050, Train MAE: 0.4594, Test MAE: 0.8866\n",
      "Epoch: 1060, Train MAE: 0.4332, Test MAE: 0.8866\n",
      "Epoch: 1070, Train MAE: 0.4566, Test MAE: 0.8995\n",
      "Epoch: 1080, Train MAE: 0.4526, Test MAE: 0.8766\n",
      "Epoch: 1090, Train MAE: 0.4494, Test MAE: 0.8930\n",
      "Epoch: 1100, Train MAE: 0.4424, Test MAE: 0.8957\n",
      "Epoch: 1110, Train MAE: 0.4472, Test MAE: 0.8786\n",
      "Epoch: 1120, Train MAE: 0.4578, Test MAE: 0.8952\n",
      "Epoch: 1130, Train MAE: 0.4476, Test MAE: 0.8938\n",
      "Epoch: 1140, Train MAE: 0.4578, Test MAE: 0.9050\n",
      "Epoch: 1150, Train MAE: 0.4500, Test MAE: 0.8917\n",
      "Epoch: 1160, Train MAE: 0.4399, Test MAE: 0.8921\n",
      "Epoch: 1170, Train MAE: 0.4387, Test MAE: 0.9028\n",
      "Epoch: 1180, Train MAE: 0.4303, Test MAE: 0.8848\n",
      "Epoch: 1190, Train MAE: 0.4350, Test MAE: 0.8938\n",
      "Epoch: 1200, Train MAE: 0.4362, Test MAE: 0.8985\n",
      "Epoch: 1210, Train MAE: 0.4334, Test MAE: 0.8967\n",
      "Epoch: 1220, Train MAE: 0.4404, Test MAE: 0.8917\n",
      "Epoch: 1230, Train MAE: 0.4424, Test MAE: 0.9042\n",
      "Epoch: 1240, Train MAE: 0.4609, Test MAE: 0.9069\n",
      "Epoch: 1250, Train MAE: 0.4306, Test MAE: 0.8923\n",
      "Epoch: 1260, Train MAE: 0.4315, Test MAE: 0.8923\n",
      "Epoch: 1270, Train MAE: 0.4216, Test MAE: 0.9000\n",
      "Epoch: 1280, Train MAE: 0.4454, Test MAE: 0.9002\n",
      "Epoch: 1290, Train MAE: 0.4226, Test MAE: 0.8811\n",
      "Epoch: 1300, Train MAE: 0.4289, Test MAE: 0.9000\n",
      "Epoch: 1310, Train MAE: 0.4590, Test MAE: 0.9013\n",
      "Epoch: 1320, Train MAE: 0.4371, Test MAE: 0.9032\n",
      "Epoch: 1330, Train MAE: 0.4533, Test MAE: 0.8974\n",
      "Epoch: 1340, Train MAE: 0.4268, Test MAE: 0.8901\n",
      "Epoch: 1350, Train MAE: 0.4362, Test MAE: 0.8924\n",
      "Epoch: 1360, Train MAE: 0.4308, Test MAE: 0.8862\n",
      "Epoch: 1370, Train MAE: 0.4365, Test MAE: 0.8918\n",
      "Epoch: 1380, Train MAE: 0.4325, Test MAE: 0.9008\n",
      "Epoch: 1390, Train MAE: 0.4326, Test MAE: 0.8932\n",
      "Epoch: 1400, Train MAE: 0.4289, Test MAE: 0.8887\n",
      "Epoch: 1410, Train MAE: 0.4450, Test MAE: 0.8866\n",
      "Epoch: 1420, Train MAE: 0.4454, Test MAE: 0.8795\n",
      "Epoch: 1430, Train MAE: 0.4302, Test MAE: 0.8932\n",
      "Epoch: 1440, Train MAE: 0.4287, Test MAE: 0.8885\n",
      "Epoch: 1450, Train MAE: 0.4367, Test MAE: 0.8939\n",
      "Epoch: 1460, Train MAE: 0.4399, Test MAE: 0.8870\n",
      "Epoch: 1470, Train MAE: 0.4121, Test MAE: 0.8794\n",
      "Epoch: 1480, Train MAE: 0.4374, Test MAE: 0.8826\n",
      "Epoch: 1490, Train MAE: 0.4318, Test MAE: 0.8915\n",
      "Epoch: 1500, Train MAE: 0.4257, Test MAE: 0.8913\n",
      "Epoch: 1510, Train MAE: 0.4510, Test MAE: 0.8818\n",
      "Epoch: 1520, Train MAE: 0.4368, Test MAE: 0.8802\n",
      "Epoch: 1530, Train MAE: 0.4219, Test MAE: 0.8866\n",
      "Epoch: 1540, Train MAE: 0.4414, Test MAE: 0.9017\n",
      "Epoch: 1550, Train MAE: 0.4259, Test MAE: 0.9010\n",
      "Epoch: 1560, Train MAE: 0.4149, Test MAE: 0.8802\n",
      "Epoch: 1570, Train MAE: 0.4203, Test MAE: 0.8877\n",
      "Epoch: 1580, Train MAE: 0.4065, Test MAE: 0.8889\n",
      "Epoch: 1590, Train MAE: 0.4227, Test MAE: 0.8900\n",
      "Epoch: 1600, Train MAE: 0.4170, Test MAE: 0.8798\n",
      "Epoch: 1610, Train MAE: 0.4241, Test MAE: 0.8855\n",
      "Epoch: 1620, Train MAE: 0.4222, Test MAE: 0.8857\n",
      "Epoch: 1630, Train MAE: 0.4125, Test MAE: 0.8921\n",
      "Epoch: 1640, Train MAE: 0.4231, Test MAE: 0.8929\n",
      "Epoch: 1650, Train MAE: 0.4315, Test MAE: 0.8924\n",
      "Epoch: 1660, Train MAE: 0.4278, Test MAE: 0.8819\n",
      "Epoch: 1670, Train MAE: 0.4315, Test MAE: 0.8885\n",
      "Epoch: 1680, Train MAE: 0.4159, Test MAE: 0.8942\n",
      "Epoch: 1690, Train MAE: 0.4288, Test MAE: 0.8876\n",
      "Epoch: 1700, Train MAE: 0.4247, Test MAE: 0.8872\n",
      "Epoch: 1710, Train MAE: 0.4238, Test MAE: 0.9049\n",
      "Epoch: 1720, Train MAE: 0.4251, Test MAE: 0.8845\n",
      "Epoch: 1730, Train MAE: 0.4188, Test MAE: 0.8954\n",
      "Epoch: 1740, Train MAE: 0.4241, Test MAE: 0.8913\n",
      "Epoch: 1750, Train MAE: 0.4552, Test MAE: 0.9018\n",
      "Epoch: 1760, Train MAE: 0.4241, Test MAE: 0.8866\n",
      "Epoch: 1770, Train MAE: 0.4365, Test MAE: 0.9036\n",
      "Epoch: 1780, Train MAE: 0.4228, Test MAE: 0.8863\n",
      "Epoch: 1790, Train MAE: 0.4170, Test MAE: 0.9051\n",
      "Epoch: 1800, Train MAE: 0.4313, Test MAE: 0.8929\n",
      "Epoch: 1810, Train MAE: 0.4253, Test MAE: 0.8891\n",
      "Epoch: 1820, Train MAE: 0.4274, Test MAE: 0.8965\n",
      "Epoch: 1830, Train MAE: 0.4407, Test MAE: 0.9082\n",
      "Epoch: 1840, Train MAE: 0.4261, Test MAE: 0.8848\n",
      "Epoch: 1850, Train MAE: 0.4147, Test MAE: 0.8884\n",
      "Epoch: 1860, Train MAE: 0.4164, Test MAE: 0.8921\n",
      "Epoch: 1870, Train MAE: 0.4045, Test MAE: 0.8966\n",
      "Epoch: 1880, Train MAE: 0.4320, Test MAE: 0.8886\n",
      "Epoch: 1890, Train MAE: 0.4483, Test MAE: 0.8995\n",
      "Epoch: 1900, Train MAE: 0.4067, Test MAE: 0.8910\n",
      "Epoch: 1910, Train MAE: 0.4106, Test MAE: 0.8956\n",
      "Epoch: 1920, Train MAE: 0.4204, Test MAE: 0.8991\n",
      "Epoch: 1930, Train MAE: 0.4096, Test MAE: 0.8871\n",
      "Epoch: 1940, Train MAE: 0.4201, Test MAE: 0.9068\n",
      "Epoch: 1950, Train MAE: 0.4053, Test MAE: 0.9023\n",
      "Epoch: 1960, Train MAE: 0.4084, Test MAE: 0.9038\n",
      "Epoch: 1970, Train MAE: 0.4162, Test MAE: 0.8927\n",
      "Epoch: 1980, Train MAE: 0.4270, Test MAE: 0.9090\n",
      "Epoch: 1990, Train MAE: 0.4089, Test MAE: 0.8923\n",
      "Epoch: 2000, Train MAE: 0.4157, Test MAE: 0.8958\n",
      "Epoch: 2010, Train MAE: 0.4120, Test MAE: 0.8927\n",
      "Epoch: 2020, Train MAE: 0.4066, Test MAE: 0.9028\n",
      "Epoch: 2030, Train MAE: 0.4219, Test MAE: 0.8923\n",
      "Epoch: 2040, Train MAE: 0.4130, Test MAE: 0.9023\n",
      "Epoch: 2050, Train MAE: 0.4301, Test MAE: 0.8919\n",
      "Epoch: 2060, Train MAE: 0.4166, Test MAE: 0.8838\n",
      "Epoch: 2070, Train MAE: 0.4081, Test MAE: 0.9005\n",
      "Epoch: 2080, Train MAE: 0.4111, Test MAE: 0.8928\n",
      "Epoch: 2090, Train MAE: 0.4220, Test MAE: 0.8910\n",
      "Epoch: 2100, Train MAE: 0.4176, Test MAE: 0.8906\n",
      "Epoch: 2110, Train MAE: 0.4269, Test MAE: 0.8925\n",
      "Epoch: 2120, Train MAE: 0.4067, Test MAE: 0.9037\n",
      "Epoch: 2130, Train MAE: 0.4570, Test MAE: 0.9069\n",
      "Epoch: 2140, Train MAE: 0.3927, Test MAE: 0.9020\n",
      "Epoch: 2150, Train MAE: 0.4071, Test MAE: 0.8917\n",
      "Epoch: 2160, Train MAE: 0.4345, Test MAE: 0.9028\n",
      "Epoch: 2170, Train MAE: 0.4188, Test MAE: 0.9017\n",
      "Epoch: 2180, Train MAE: 0.4180, Test MAE: 0.8948\n",
      "Epoch: 2190, Train MAE: 0.4019, Test MAE: 0.9015\n",
      "Epoch: 2200, Train MAE: 0.4071, Test MAE: 0.8881\n",
      "Epoch: 2210, Train MAE: 0.4012, Test MAE: 0.8943\n",
      "Epoch: 2220, Train MAE: 0.4455, Test MAE: 0.8973\n",
      "Epoch: 2230, Train MAE: 0.4070, Test MAE: 0.8980\n",
      "Epoch: 2240, Train MAE: 0.4292, Test MAE: 0.8956\n",
      "Epoch: 2250, Train MAE: 0.4182, Test MAE: 0.8894\n",
      "Epoch: 2260, Train MAE: 0.3982, Test MAE: 0.9099\n",
      "Epoch: 2270, Train MAE: 0.4181, Test MAE: 0.8993\n",
      "Epoch: 2280, Train MAE: 0.4094, Test MAE: 0.9008\n",
      "Epoch: 2290, Train MAE: 0.4127, Test MAE: 0.8955\n",
      "Epoch: 2300, Train MAE: 0.4088, Test MAE: 0.8966\n",
      "Epoch: 2310, Train MAE: 0.3970, Test MAE: 0.8909\n",
      "Epoch: 2320, Train MAE: 0.4174, Test MAE: 0.8845\n",
      "Epoch: 2330, Train MAE: 0.4066, Test MAE: 0.8944\n",
      "Epoch: 2340, Train MAE: 0.4349, Test MAE: 0.8978\n",
      "Epoch: 2350, Train MAE: 0.4373, Test MAE: 0.9031\n",
      "Epoch: 2360, Train MAE: 0.4144, Test MAE: 0.8941\n",
      "Epoch: 2370, Train MAE: 0.4544, Test MAE: 0.8981\n",
      "Epoch: 2380, Train MAE: 0.3944, Test MAE: 0.9018\n",
      "Epoch: 2390, Train MAE: 0.4014, Test MAE: 0.9011\n",
      "Epoch: 2400, Train MAE: 0.4219, Test MAE: 0.8887\n",
      "Epoch: 2410, Train MAE: 0.4159, Test MAE: 0.8888\n",
      "Epoch: 2420, Train MAE: 0.3902, Test MAE: 0.8962\n",
      "Epoch: 2430, Train MAE: 0.4119, Test MAE: 0.8843\n",
      "Epoch: 2440, Train MAE: 0.4175, Test MAE: 0.8960\n",
      "Epoch: 2450, Train MAE: 0.4075, Test MAE: 0.9015\n",
      "Epoch: 2460, Train MAE: 0.4109, Test MAE: 0.8859\n",
      "Epoch: 2470, Train MAE: 0.4013, Test MAE: 0.8983\n",
      "Epoch: 2480, Train MAE: 0.4138, Test MAE: 0.9016\n",
      "Epoch: 2490, Train MAE: 0.3969, Test MAE: 0.8903\n",
      "Epoch: 2500, Train MAE: 0.4088, Test MAE: 0.8958\n",
      "Epoch: 2510, Train MAE: 0.4203, Test MAE: 0.8925\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch import optim\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(1)\n",
    "        \n",
    "        num_features = dataset[0].num_features\n",
    "        num_edge_features = dataset[0].num_edge_features\n",
    "        \n",
    "        \n",
    "        nn = Seq(Lin(num_edge_features, 16), ReLU(), Lin(16, dataset[0].num_node_features* 96))\n",
    "        self.conv1 = NNConv(dataset[0].num_node_features, 96, nn=nn)\n",
    "        nn = Seq(Lin(num_edge_features, 16), ReLU(), Lin(16, 96* hidden_channels))\n",
    "        self.conv2 = NNConv(96, hidden_channels, nn=nn)\n",
    "        nn = Seq(Lin(num_edge_features, 16), ReLU(), Lin(16, hidden_channels* hidden_channels))\n",
    "        self.conv3 = NNConv(hidden_channels, hidden_channels, nn=nn)\n",
    "        self.conv4 = NNConv(hidden_channels, hidden_channels, nn=nn)\n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=hidden_channels).to(device=device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_v = torch.nn.L1Loss() # that's the MAE Loss\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch,  data.edge_attr)  # Perform a single forward pass.\n",
    "        loss = criterion(out.flatten(), data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    loss = torch.Tensor([0]).to(device=device)\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch, data.edge_attr) \n",
    "        loss += criterion_v(out.flatten(), data.y)\n",
    "    return loss/len(loader) # MAE loss of batches can be summed and divided by the number of batches\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train MAE: {train_acc.item():.4f}, Test MAE: {test_acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
